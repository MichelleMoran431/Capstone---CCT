{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dcc151-ba07-4dd2-a729-8e542887a504",
   "metadata": {},
   "source": [
    "## Looking at all tanks from 23MT - 20 tonne Capacity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995b1ab1-87b0-48f6-a021-63abb8755064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Supress Warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "#The last line of code helps in suppressing the unnecessary warnings.\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeaea3fa-7f24-4922-8b17-10c4c082d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Specify Absolute Path: If the file is located in a different directory, you can specify the absolute path to the file when reading it using pd.read_csv():\n",
    "import pandas as pd\n",
    "file_path = r'C:\\Users\\User\\Desktop\\Thesis 2023\\Capstone---CCT\\Python Working Notebooks\\ProductionDataupdated1.csv'\n",
    "ProductionTank = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c0f3e12-cbc5-4462-8bdc-46fe09b0dce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Material</th>\n",
       "      <th>BATCHID</th>\n",
       "      <th>Tank_1</th>\n",
       "      <th>Instruction_Step</th>\n",
       "      <th>INGRED_ID</th>\n",
       "      <th>INGRED_Name</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Phase_start</th>\n",
       "      <th>Phase_end</th>\n",
       "      <th>Phase_duration</th>\n",
       "      <th>Phase_start_delay</th>\n",
       "      <th>Phase_row_no</th>\n",
       "      <th>Flowrate_KGMIN</th>\n",
       "      <th>Target_Flowrate</th>\n",
       "      <th>Target_Phase_duration</th>\n",
       "      <th>Phase_overrun</th>\n",
       "      <th>Deaeration Phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1002150</td>\n",
       "      <td>107643491</td>\n",
       "      <td>2503</td>\n",
       "      <td>S3_BATCH_IN_PROGRESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>09/03/2022 10:42</td>\n",
       "      <td>09/03/2022 10:42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1002150</td>\n",
       "      <td>107643491</td>\n",
       "      <td>2503</td>\n",
       "      <td>STEP1_CONS</td>\n",
       "      <td>1002565</td>\n",
       "      <td>WATER TREATED</td>\n",
       "      <td>5760.000</td>\n",
       "      <td>09/03/2022 10:42</td>\n",
       "      <td>09/03/2022 11:16</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>169.4118</td>\n",
       "      <td>733.5050</td>\n",
       "      <td>8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1002150</td>\n",
       "      <td>107643491</td>\n",
       "      <td>2503</td>\n",
       "      <td>PLEASE VERIFY BULK ADDITION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>09/03/2022 11:16</td>\n",
       "      <td>09/03/2022 11:17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1002150</td>\n",
       "      <td>107643491</td>\n",
       "      <td>2503</td>\n",
       "      <td>STEP1_CONS</td>\n",
       "      <td>1037802</td>\n",
       "      <td>S813     SOD BENZOATE          XFX25</td>\n",
       "      <td>5.629</td>\n",
       "      <td>09/03/2022 11:17</td>\n",
       "      <td>09/03/2022 11:27</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>6.3182</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1002150</td>\n",
       "      <td>107643491</td>\n",
       "      <td>2503</td>\n",
       "      <td>STEP1_CONS</td>\n",
       "      <td>1002818</td>\n",
       "      <td>S651     CITRIC ACID ANH    BG XFX25</td>\n",
       "      <td>78.766</td>\n",
       "      <td>09/03/2022 11:27</td>\n",
       "      <td>09/03/2022 11:38</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.1605</td>\n",
       "      <td>6.3182</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>9482</td>\n",
       "      <td>3055706</td>\n",
       "      <td>107737576</td>\n",
       "      <td>2502</td>\n",
       "      <td>TAKE A SAMPLE AND SUBMIT FOR QA.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>08/05/2022 11:43</td>\n",
       "      <td>08/05/2022 11:54</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9483</th>\n",
       "      <td>9483</td>\n",
       "      <td>3055706</td>\n",
       "      <td>107737576</td>\n",
       "      <td>2502</td>\n",
       "      <td>SAMPLE TO LAB. RESULTS OK? (NO TO HOMOGENISE)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>08/05/2022 11:54</td>\n",
       "      <td>08/05/2022 11:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9484</th>\n",
       "      <td>9484</td>\n",
       "      <td>3055706</td>\n",
       "      <td>107737576</td>\n",
       "      <td>2502</td>\n",
       "      <td>STEP8_AGITATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>08/05/2022 11:56</td>\n",
       "      <td>08/05/2022 11:56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9485</th>\n",
       "      <td>9485</td>\n",
       "      <td>3055706</td>\n",
       "      <td>107737576</td>\n",
       "      <td>2502</td>\n",
       "      <td>S4_BATCH_COMPLETE_QA_PENDING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>08/05/2022 11:56</td>\n",
       "      <td>08/05/2022 11:56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9486</th>\n",
       "      <td>9486</td>\n",
       "      <td>3055706</td>\n",
       "      <td>107737576</td>\n",
       "      <td>2502</td>\n",
       "      <td>S7_RELEASED_TO_FILLING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>08/05/2022 12:02</td>\n",
       "      <td>08/05/2022 12:02</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9487 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Material    BATCHID Tank_1  \\\n",
       "0              0   1002150  107643491   2503   \n",
       "1              1   1002150  107643491   2503   \n",
       "2              2   1002150  107643491   2503   \n",
       "3              3   1002150  107643491   2503   \n",
       "4              4   1002150  107643491   2503   \n",
       "...          ...       ...        ...    ...   \n",
       "9482        9482   3055706  107737576   2502   \n",
       "9483        9483   3055706  107737576   2502   \n",
       "9484        9484   3055706  107737576   2502   \n",
       "9485        9485   3055706  107737576   2502   \n",
       "9486        9486   3055706  107737576   2502   \n",
       "\n",
       "                                   Instruction_Step INGRED_ID  \\\n",
       "0                              S3_BATCH_IN_PROGRESS       NaN   \n",
       "1                                        STEP1_CONS   1002565   \n",
       "2                       PLEASE VERIFY BULK ADDITION       NaN   \n",
       "3                                        STEP1_CONS   1037802   \n",
       "4                                        STEP1_CONS   1002818   \n",
       "...                                             ...       ...   \n",
       "9482               TAKE A SAMPLE AND SUBMIT FOR QA.       NaN   \n",
       "9483  SAMPLE TO LAB. RESULTS OK? (NO TO HOMOGENISE)       NaN   \n",
       "9484                                STEP8_AGITATION       NaN   \n",
       "9485                   S4_BATCH_COMPLETE_QA_PENDING       NaN   \n",
       "9486                         S7_RELEASED_TO_FILLING       NaN   \n",
       "\n",
       "                               INGRED_Name  Quantity       Phase_start  \\\n",
       "0                                      NaN     0.000  09/03/2022 10:42   \n",
       "1                            WATER TREATED  5760.000  09/03/2022 10:42   \n",
       "2                                      NaN     0.000  09/03/2022 11:16   \n",
       "3     S813     SOD BENZOATE          XFX25     5.629  09/03/2022 11:17   \n",
       "4     S651     CITRIC ACID ANH    BG XFX25    78.766  09/03/2022 11:27   \n",
       "...                                    ...       ...               ...   \n",
       "9482                                   NaN     0.000  08/05/2022 11:43   \n",
       "9483                                   NaN     0.000  08/05/2022 11:54   \n",
       "9484                                   NaN     0.000  08/05/2022 11:56   \n",
       "9485                                   NaN     0.000  08/05/2022 11:56   \n",
       "9486                                   NaN     0.000  08/05/2022 12:02   \n",
       "\n",
       "             Phase_end  Phase_duration  Phase_start_delay  Phase_row_no  \\\n",
       "0     09/03/2022 10:42               0                  0             1   \n",
       "1     09/03/2022 11:16              34                  0             2   \n",
       "2     09/03/2022 11:17               1                  0             3   \n",
       "3     09/03/2022 11:27              10                  0             4   \n",
       "4     09/03/2022 11:38              11                  0             5   \n",
       "...                ...             ...                ...           ...   \n",
       "9482  08/05/2022 11:54              11                  0            19   \n",
       "9483  08/05/2022 11:55               1                  0            20   \n",
       "9484  08/05/2022 11:56               0                  1            21   \n",
       "9485  08/05/2022 11:56               0                  0            22   \n",
       "9486  08/05/2022 12:02               0                  6            23   \n",
       "\n",
       "      Flowrate_KGMIN  Target_Flowrate  Target_Phase_duration  Phase_overrun  \\\n",
       "0             0.0000              NaN                      0            NaN   \n",
       "1           169.4118         733.5050                      8           26.0   \n",
       "2             0.0000              NaN                      3            0.0   \n",
       "3             0.5629           6.3182                      1            9.0   \n",
       "4             7.1605           6.3182                     12            0.0   \n",
       "...              ...              ...                    ...            ...   \n",
       "9482          0.0000              NaN                     10            1.0   \n",
       "9483          0.0000              NaN                     10            0.0   \n",
       "9484          0.0000              NaN                      0            0.0   \n",
       "9485          0.0000              NaN                      0            NaN   \n",
       "9486          0.0000              NaN                     14            0.0   \n",
       "\n",
       "      Deaeration Phase  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "9482                 0  \n",
       "9483                 0  \n",
       "9484                 0  \n",
       "9485                 0  \n",
       "9486                 0  \n",
       "\n",
       "[9487 rows x 18 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProductionTank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f004067c-b57d-42c7-af68-8becb4f2f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductionTank.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2aeb92-6db6-461e-a672-7cc463fc741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Material    BATCHID Tank_1  \\\n",
      "1932   1397022  107899894   2302   \n",
      "1933   1397022  107899894   2302   \n",
      "1934   1397022  107899894   2302   \n",
      "1935   1397022  107899894   2302   \n",
      "1936   1397022  107899894   2302   \n",
      "...        ...        ...    ...   \n",
      "8015   1775253  108084747   2304   \n",
      "8016   1775253  108084747   2304   \n",
      "8017   1775253  108084747   2304   \n",
      "8018   1775253  108084747   2304   \n",
      "8019   1775253  108084747   2304   \n",
      "\n",
      "                                       Instruction_Step INGRED_ID  \\\n",
      "1932                               S3_BATCH_IN_PROGRESS       NaN   \n",
      "1933                                         GUM_PROMPT       NaN   \n",
      "1934  PROCEED WHEN SAMPLE IS DEAREATED AND DENSITY I...       NaN   \n",
      "1935                                         STEP1_CONS   1002565   \n",
      "1936                        PLEASE VERIFY BULK ADDITION       NaN   \n",
      "...                                                 ...       ...   \n",
      "8015                       S4_BATCH_COMPLETE_QA_PENDING       NaN   \n",
      "8016                   TAKE A SAMPLE AND SUBMIT FOR QA.       NaN   \n",
      "8017      SAMPLE TO LAB. RESULTS OK? (NO TO HOMOGENISE)       NaN   \n",
      "8018                       S4_BATCH_COMPLETE_QA_PENDING       NaN   \n",
      "8019                             S7_RELEASED_TO_FILLING       NaN   \n",
      "\n",
      "        INGRED_Name  Quantity       Phase_start         Phase_end  \\\n",
      "1932            NaN       0.0  15/09/2022 10:59  15/09/2022 10:59   \n",
      "1933            NaN       0.0  15/09/2022 10:59  15/09/2022 11:10   \n",
      "1934            NaN       0.0  15/09/2022 11:11  15/09/2022 11:11   \n",
      "1935  WATER TREATED    8640.0  15/09/2022 11:12  15/09/2022 11:22   \n",
      "1936            NaN       0.0  15/09/2022 11:22  15/09/2022 11:29   \n",
      "...             ...       ...               ...               ...   \n",
      "8015            NaN       0.0  05/03/2023 09:07  05/03/2023 09:07   \n",
      "8016            NaN       0.0  05/03/2023 09:07  05/03/2023 09:28   \n",
      "8017            NaN       0.0  05/03/2023 09:28  05/03/2023 09:28   \n",
      "8018            NaN       0.0  05/03/2023 09:28  05/03/2023 09:29   \n",
      "8019            NaN       0.0  05/03/2023 10:30  05/03/2023 10:31   \n",
      "\n",
      "      Phase_duration  Phase_start_delay  Phase_row_no  Flowrate_KGMIN  \\\n",
      "1932               0                  0             1             0.0   \n",
      "1933              11                  0             2             0.0   \n",
      "1934               0                  1             3             0.0   \n",
      "1935              10                  1             4           864.0   \n",
      "1936               7                  0             5             0.0   \n",
      "...              ...                ...           ...             ...   \n",
      "8015               0                  1             2             0.0   \n",
      "8016              21                  0             3             0.0   \n",
      "8017               0                  0             4             0.0   \n",
      "8018               1                  0             5             0.0   \n",
      "8019               1                 61             6             0.0   \n",
      "\n",
      "      Target_Flowrate  Target_Phase_duration  Phase_overrun  Deaeration Phase  \n",
      "1932              NaN                      0            NaN                 0  \n",
      "1933              NaN                      3            8.0                 0  \n",
      "1934              NaN                     10            0.0                 0  \n",
      "1935          733.505                     12            0.0                 0  \n",
      "1936              NaN                      3            4.0                 0  \n",
      "...               ...                    ...            ...               ...  \n",
      "8015              NaN                      0            NaN                 0  \n",
      "8016              NaN                     10           11.0                 0  \n",
      "8017              NaN                     10            0.0                 0  \n",
      "8018              NaN                      1            NaN                 0  \n",
      "8019              NaN                     14            1.0                 0  \n",
      "\n",
      "[2158 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "tanks = ('2302','2304','2305')  # List of tank IDs you want to query\n",
    "ProductionTanks_df = ProductionTank.query('Tank_1 in @tanks')\n",
    "print(ProductionTanks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e16d7fbf-8c55-4875-9773-f284f30d1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0   107548331             377           61.0          99.961538   \n",
      "1   107553570             382           88.0         104.368421   \n",
      "2   107553571             614          282.0         169.120000   \n",
      "3   107569832            1283          833.0         129.521739   \n",
      "4   107569833             802          282.0         130.068966   \n",
      "..        ...             ...            ...                ...   \n",
      "89  108067816             738          273.0         158.600000   \n",
      "90  108067817             289           12.0           0.545455   \n",
      "91  108084745             456          190.0         184.150000   \n",
      "92  108084746             482          121.0         164.692308   \n",
      "93  108084747             459           80.0         173.153846   \n",
      "\n",
      "    Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0        1526.2458              14.269231       169.251889  \n",
      "1        1155.8386              16.473684       169.394162  \n",
      "2        1883.9474              14.720000       202.992530  \n",
      "3        2092.1390              20.695652       259.572162  \n",
      "4        2045.5026              19.965517       259.572162  \n",
      "..             ...                    ...              ...  \n",
      "89       1835.2925              19.960000       246.789922  \n",
      "90        226.1160              29.090909       237.850400  \n",
      "91       1972.1571              15.450000       247.907363  \n",
      "92       2103.3979              16.576923       247.907363  \n",
      "93       2087.7102              16.423077       247.907363  \n",
      "\n",
      "[94 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Aggregate data per tank\n",
    "aggregated_ProductionTank23_df1 = ProductionTanks_df.groupby(['BATCHID']).agg({\n",
    "    'Phase_duration': 'sum',\n",
    "    'Phase_overrun': 'sum',\n",
    "    'Phase_start_delay':'mean',\n",
    "    #'Quantity':'sum',\n",
    "    'Flowrate_KGMIN':'sum',\n",
    "    'Target_Phase_duration':'mean',\n",
    "    'Target_Flowrate':'mean'\n",
    "}).reset_index()\n",
    "\n",
    " #Print the aggregated DataFrame\n",
    "print(aggregated_ProductionTank23_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aa77add-b0fd-48fd-bffa-bc4f4fb3dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "aggregated_ProductionTank23_df1.dropna(inplace=True)  # Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a213668-1f1f-49a8-9515-39cc2db20641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicates\n",
    "aggregated_ProductionTank23_df1.drop_duplicates(inplace=True)  # Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db6cd17e-b655-4ef1-8ba5-ade69437a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0   107548331             377           61.0          99.961538   \n",
      "1   107553570             382           88.0         104.368421   \n",
      "2   107553571             614          282.0         169.120000   \n",
      "4   107569833             802          282.0         130.068966   \n",
      "5   107579750             412          174.0         146.666667   \n",
      "..        ...             ...            ...                ...   \n",
      "88  108067815             716          314.0         134.230769   \n",
      "89  108067816             738          273.0         158.600000   \n",
      "91  108084745             456          190.0         184.150000   \n",
      "92  108084746             482          121.0         164.692308   \n",
      "93  108084747             459           80.0         173.153846   \n",
      "\n",
      "    Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0        1526.2458              14.269231       169.251889  \n",
      "1        1155.8386              16.473684       169.394162  \n",
      "2        1883.9474              14.720000       202.992530  \n",
      "4        2045.5026              19.965517       259.572162  \n",
      "5        1767.2573              14.388889       263.857629  \n",
      "..             ...                    ...              ...  \n",
      "88       1630.0695              17.307692       239.717863  \n",
      "89       1835.2925              19.960000       246.789922  \n",
      "91       1972.1571              15.450000       247.907363  \n",
      "92       2103.3979              16.576923       247.907363  \n",
      "93       2087.7102              16.423077       247.907363  \n",
      "\n",
      "[71 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define columns where you want to detect and remove outliers\n",
    "ProductionTank23_df2 = pd.DataFrame(aggregated_ProductionTank23_df1)\n",
    "columns_to_check = ['Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN', 'Target_Phase_duration', 'Target_Flowrate']\n",
    "\n",
    "# Define a function to remove outliers using IQR\n",
    "def remove_outliers_iqr(data, column, iqr_multiplier=1.5):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers for each column\n",
    "for col in columns_to_check:\n",
    "    ProductionTank23_df2 = remove_outliers_iqr(ProductionTank23_df2, col)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(ProductionTank23_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a7bb26-b28e-4fc9-91bd-632857e04200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Summary Statistics:\n",
      "            BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "count  9.000000e+01       90.000000      90.000000          90.000000   \n",
      "mean   1.077811e+08      765.677778     359.133333         120.232767   \n",
      "std    1.381263e+05      428.444530     335.259624          83.631751   \n",
      "min    1.075483e+08      184.000000       0.000000           0.400000   \n",
      "25%    1.076715e+08      520.250000     181.000000          70.644783   \n",
      "50%    1.077818e+08      683.000000     274.000000         128.328235   \n",
      "75%    1.078565e+08      836.250000     430.500000         164.206731   \n",
      "max    1.080847e+08     2749.000000    2095.000000         652.000000   \n",
      "\n",
      "       Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "count       90.000000              90.000000        90.000000  \n",
      "mean      1694.968833              23.224562       219.906519  \n",
      "std        583.154246              46.884201        47.176096  \n",
      "min        176.592300              10.260870        34.382650  \n",
      "25%       1634.070875              15.450000       173.428742  \n",
      "50%       1839.472500              16.850323       237.850400  \n",
      "75%       2058.765775              20.288387       247.907363  \n",
      "max       2518.623400             460.500000       304.204483  \n",
      "\n",
      "Cleaned DataFrame Summary Statistics:\n",
      "            BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "count  7.100000e+01       71.000000      71.000000          71.000000   \n",
      "mean   1.077762e+08      694.732394     300.943662         129.394587   \n",
      "std    1.424374e+05      190.245327     142.783441          51.726921   \n",
      "min    1.075483e+08      377.000000      52.000000          24.652174   \n",
      "25%    1.076599e+08      558.500000     197.000000          92.974545   \n",
      "50%    1.077716e+08      682.000000     275.000000         134.230769   \n",
      "75%    1.078565e+08      814.500000     385.500000         167.754079   \n",
      "max    1.080847e+08     1218.000000     638.000000         228.347826   \n",
      "\n",
      "       Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "count       71.000000              71.000000        71.000000  \n",
      "mean      1883.807023              17.158462       221.884494  \n",
      "std        275.676622               2.789550        44.506143  \n",
      "min       1155.838600              10.260870       143.302833  \n",
      "25%       1721.862100              15.450000       173.168660  \n",
      "50%       1882.116100              16.473684       239.717863  \n",
      "75%       2085.687450              19.931613       253.739763  \n",
      "max       2518.623400              22.480000       304.204483  \n"
     ]
    }
   ],
   "source": [
    "# For the original DataFrame\n",
    "print(\"Original DataFrame Summary Statistics:\")\n",
    "print(aggregated_ProductionTank23_df1.describe())\n",
    "\n",
    "# After removing outliers\n",
    "print(\"\\nCleaned DataFrame Summary Statistics:\")\n",
    "print(ProductionTank23_df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73378539-de89-4434-b63c-51249f8a9df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0  107548331       -0.912266      -0.894243          -0.243745   \n",
      "1  107553570       -0.900530      -0.813257          -0.190756   \n",
      "2  107553571       -0.356003      -0.231359           0.587828   \n",
      "3  107569832        1.214207       1.421350           0.111692   \n",
      "4  107569833        0.085252      -0.231359           0.118272   \n",
      "\n",
      "   Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0       -0.290949              14.269231       169.251889  \n",
      "1       -0.929686              16.473684       169.394162  \n",
      "2        0.325878              14.720000       202.992530  \n",
      "3        0.684888              20.695652       259.572162  \n",
      "4        0.604467              19.965517       259.572162  \n"
     ]
    }
   ],
   "source": [
    "# Scaling numerical variables (if needed)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN']\n",
    "aggregated_ProductionTank23_df1[numerical_cols] = scaler.fit_transform(aggregated_ProductionTank23_df1[numerical_cols])\n",
    "print(aggregated_ProductionTank23_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b54a05-5b89-4043-be08-937efb68029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0  107548331       -1.682006      -1.692433          -0.573058   \n",
      "1  107553570       -1.655538      -1.501990          -0.487257   \n",
      "2  107553571       -0.427380      -0.133618           0.773449   \n",
      "4  107569833        0.567851      -0.133618           0.013130   \n",
      "5  107579750       -1.496724      -0.895392           0.336286   \n",
      "\n",
      "   Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0       -1.306263              14.269231       169.251889  \n",
      "1       -2.659456              16.473684       169.394162  \n",
      "2        0.000513              14.720000       202.992530  \n",
      "4        0.590715              19.965517       259.572162  \n",
      "5       -0.425786              14.388889       263.857629  \n"
     ]
    }
   ],
   "source": [
    "# Scaling numerical variables (if needed)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN']\n",
    "ProductionTank23_df2[numerical_cols] = scaler.fit_transform(ProductionTank23_df2[numerical_cols])\n",
    "print(ProductionTank23_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfa2eb81-b103-4ae6-867b-e87cec8a9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|    | Model                       |   Train MSE |   Test MSE |   Train R2 |   Test R2 |\n",
      "+====+=============================+=============+============+============+===========+\n",
      "|  0 | Linear Regression           | 0.0464037   |   0.121318 |  0.953046  |  0.838182 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  1 | Ridge Regression            | 0.0470393   |   0.114792 |  0.952403  |  0.846886 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  2 | Lasso Regression            | 0.89553     |   1.32509  |  0.0938465 | -0.767459 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  3 | Random Forest Regressor     | 0.025646    |   0.38941  |  0.97405   |  0.480591 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  4 | Gradient Boosting Regressor | 0.000633303 |   0.219177 |  0.999359  |  0.707654 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  5 | Decision Tree Regressor     | 0           |   0.250219 |  1         |  0.666248 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  6 | Bagging Regressor           | 0.0250737   |   0.400644 |  0.974629  |  0.465606 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n",
      "|  7 | AdaBoost Regressor          | 0.0300722   |   0.36242  |  0.969571  |  0.516591 |\n",
      "+----+-----------------------------+-------------+------------+------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank22_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank23_df2)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred_train = lr_model.predict(X_train)\n",
    "lr_pred_test = lr_model.predict(X_test)\n",
    "lr_train_mse = mean_squared_error(y_train, lr_pred_train)\n",
    "lr_test_mse = mean_squared_error(y_test, lr_pred_test)\n",
    "lr_train_r2 = r2_score(y_train, lr_pred_train)\n",
    "lr_test_r2 = r2_score(y_test, lr_pred_test)\n",
    "results_df = results_df.append({'Model': 'Linear Regression', 'Train MSE': lr_train_mse, 'Test MSE': lr_test_mse, 'Train R2': lr_train_r2, 'Test R2': lr_test_r2}, ignore_index=True)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_pred_train = ridge_model.predict(X_train)\n",
    "ridge_pred_test = ridge_model.predict(X_test)\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_pred_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_pred_test)\n",
    "ridge_train_r2 = r2_score(y_train, ridge_pred_train)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_pred_test)\n",
    "results_df = results_df.append({'Model': 'Ridge Regression', 'Train MSE': ridge_train_mse, 'Test MSE': ridge_test_mse, 'Train R2': ridge_train_r2, 'Test R2': ridge_test_r2}, ignore_index=True)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "lasso_pred_train = lasso_model.predict(X_train)\n",
    "lasso_pred_test = lasso_model.predict(X_test)\n",
    "lasso_train_mse = mean_squared_error(y_train, lasso_pred_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_pred_test)\n",
    "lasso_train_r2 = r2_score(y_train, lasso_pred_train)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_pred_test)\n",
    "results_df = results_df.append({'Model': 'Lasso Regression', 'Train MSE': lasso_train_mse, 'Test MSE': lasso_test_mse, 'Train R2': lasso_train_r2, 'Test R2': lasso_test_r2}, ignore_index=True)\n",
    "\n",
    "# RandomForest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "rf_train_mse = mean_squared_error(y_train, rf_pred_train)\n",
    "rf_test_mse = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_train_r2 = r2_score(y_train, rf_pred_train)\n",
    "rf_test_r2 = r2_score(y_test, rf_pred_test)\n",
    "results_df = results_df.append({'Model': 'Random Forest Regressor', 'Train MSE': rf_train_mse, 'Test MSE': rf_test_mse, 'Train R2': rf_train_r2, 'Test R2': rf_test_r2}, ignore_index=True)\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred_train = gb_model.predict(X_train)\n",
    "gb_pred_test = gb_model.predict(X_test)\n",
    "gb_train_mse = mean_squared_error(y_train, gb_pred_train)\n",
    "gb_test_mse = mean_squared_error(y_test, gb_pred_test)\n",
    "gb_train_r2 = r2_score(y_train, gb_pred_train)\n",
    "gb_test_r2 = r2_score(y_test, gb_pred_test)\n",
    "results_df = results_df.append({'Model': 'Gradient Boosting Regressor', 'Train MSE': gb_train_mse, 'Test MSE': gb_test_mse, 'Train R2': gb_train_r2, 'Test R2': gb_test_r2}, ignore_index=True)\n",
    "gb_feature_importance = gb_model.feature_importances_\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred_train = dt_model.predict(X_train)\n",
    "dt_pred_test = dt_model.predict(X_test)\n",
    "dt_train_mse = mean_squared_error(y_train, dt_pred_train)\n",
    "dt_test_mse = mean_squared_error(y_test, dt_pred_test)\n",
    "dt_train_r2 = r2_score(y_train, dt_pred_train)\n",
    "dt_test_r2 = r2_score(y_test, dt_pred_test)\n",
    "results_df = results_df.append({'Model': 'Decision Tree Regressor', 'Train MSE': dt_train_mse, 'Test MSE': dt_test_mse, 'Train R2': dt_train_r2, 'Test R2': dt_test_r2}, ignore_index=True)\n",
    "\n",
    "# Bagging Regressor (based on Decision Trees by default)\n",
    "bag_model = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "bag_model.fit(X_train, y_train)\n",
    "bag_pred_train = bag_model.predict(X_train)\n",
    "bag_pred_test = bag_model.predict(X_test)\n",
    "bag_train_mse = mean_squared_error(y_train, bag_pred_train)\n",
    "bag_test_mse = mean_squared_error(y_test, bag_pred_test)\n",
    "bag_train_r2 = r2_score(y_train, bag_pred_train)\n",
    "bag_test_r2 = r2_score(y_test, bag_pred_test)\n",
    "results_df = results_df.append({'Model': 'Bagging Regressor', 'Train MSE': bag_train_mse, 'Test MSE': bag_test_mse, 'Train R2': bag_train_r2, 'Test R2': bag_test_r2}, ignore_index=True)\n",
    "\n",
    "# AdaBoost Regressor\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred_train = ada_model.predict(X_train)\n",
    "ada_pred_test = ada_model.predict(X_test)\n",
    "ada_train_mse = mean_squared_error(y_train, ada_pred_train)\n",
    "ada_test_mse = mean_squared_error(y_test, ada_pred_test)\n",
    "ada_train_r2 = r2_score(y_train, ada_pred_train)\n",
    "ada_test_r2 = r2_score(y_test, ada_pred_test)\n",
    "results_df = results_df.append({'Model': 'AdaBoost Regressor', 'Train MSE': ada_train_mse, 'Test MSE': ada_test_mse, 'Train R2': ada_train_r2, 'Test R2': ada_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Print the results DataFrame\n",
    "#print(results_df)\n",
    "# Print the results DataFrame in tabulated form\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('23results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "794b04f8-797b-4883-a7d0-9d12e4cba5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression:\n",
      "  Mean MSE: 0.072895\n",
      "  Std MSE: 0.006640\n",
      "\n",
      "Ridge:\n",
      "  Mean MSE: 0.073355\n",
      "  Std MSE: 0.008022\n",
      "\n",
      "Lasso:\n",
      "  Mean MSE: 1.333379\n",
      "  Std MSE: 0.566640\n",
      "\n",
      "RandomForestRegressor:\n",
      "  Mean MSE: 0.239007\n",
      "  Std MSE: 0.125224\n",
      "\n",
      "GradientBoostingRegressor:\n",
      "  Mean MSE: 0.220306\n",
      "  Std MSE: 0.080704\n",
      "\n",
      "SVR:\n",
      "  Mean MSE: 1.072575\n",
      "  Std MSE: 0.237829\n",
      "\n",
      "MLPRegressor:\n",
      "  Mean MSE: 4334132507173.814941\n",
      "  Std MSE: 5025015959858.388672\n",
      "\n",
      "DecisionTreeRegressor:\n",
      "  Mean MSE: 0.327271\n",
      "  Std MSE: 0.096998\n",
      "\n",
      "AdaBoostRegressor:\n",
      "  Mean MSE: 0.249539\n",
      "  Std MSE: 0.104848\n",
      "\n",
      "BaggingRegressor:\n",
      "  Mean MSE: 0.231017\n",
      "  Std MSE: 0.110139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of models with their respective hyperparameters\n",
    "# Initialize models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(),\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    BaggingRegressor(n_estimators=100, random_state=42)\n",
    "]\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_scores = -scores  # Convert negative MSE back to positive\n",
    "    mean_mse = mse_scores.mean()\n",
    "    std_mse = mse_scores.std()\n",
    "    print(f\"{model_name}:\\n  Mean MSE: {mean_mse:.6f}\\n  Std MSE: {std_mse:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61087bc4-c75a-4f2a-832a-bbd1571984bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Load your dataset (replace 'ProductionTank2202_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank23_df2)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred_train = lr_model.predict(X_train)\n",
    "lr_pred_test = lr_model.predict(X_test)\n",
    "lr_train_mse = mean_squared_error(y_train, lr_pred_train)\n",
    "lr_test_mse = mean_squared_error(y_test, lr_pred_test)\n",
    "lr_train_r2 = r2_score(y_train, lr_pred_train)\n",
    "lr_test_r2 = r2_score(y_test, lr_pred_test)\n",
    "results_df = results_df.append({'Model': 'Linear Regression', 'Train MSE': lr_train_mse, 'Test MSE': lr_test_mse, 'Train R2': lr_train_r2, 'Test R2': lr_test_r2}, ignore_index=True)\n",
    "\n",
    "# Ridge Regression with Hyperparameter Tuning\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "ridge_pred_train = best_ridge.predict(X_train)\n",
    "ridge_pred_test = best_ridge.predict(X_test)\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_pred_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_pred_test)\n",
    "ridge_train_r2 = r2_score(y_train, ridge_pred_train)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_pred_test)\n",
    "results_df = results_df.append({'Model': 'Ridge Regression', 'Train MSE': ridge_train_mse, 'Test MSE': ridge_test_mse, 'Train R2': ridge_train_r2, 'Test R2': ridge_test_r2}, ignore_index=True)\n",
    "\n",
    "# Lasso Regression with Hyperparameter Tuning\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
    "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=5)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "lasso_pred_train = best_lasso.predict(X_train)\n",
    "lasso_pred_test = best_lasso.predict(X_test)\n",
    "lasso_train_mse = mean_squared_error(y_train, lasso_pred_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_pred_test)\n",
    "lasso_train_r2 = r2_score(y_train, lasso_pred_train)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_pred_test)\n",
    "results_df = results_df.append({'Model': 'Lasso Regression', 'Train MSE': lasso_train_mse, 'Test MSE': lasso_test_mse, 'Train R2': lasso_train_r2, 'Test R2': lasso_test_r2}, ignore_index=True)\n",
    "\n",
    "# Random Forest Regressor with Hyperparameter Tuning\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20]}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(), rf_params, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "rf_pred_train = best_rf.predict(X_train)\n",
    "rf_pred_test = best_rf.predict(X_test)\n",
    "rf_train_mse = mean_squared_error(y_train, rf_pred_train)\n",
    "rf_test_mse = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_train_r2 = r2_score(y_train, rf_pred_train)\n",
    "rf_test_r2 = r2_score(y_test, rf_pred_test)\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "results_df = results_df.append({'Model': 'Random Forest Regressor', 'Train MSE': rf_train_mse, 'Test MSE': rf_test_mse, 'Train R2': rf_train_r2, 'Test R2': rf_test_r2}, ignore_index=True)\n",
    "\n",
    "# Gradient Boosting Regressor with Hyperparameter Tuning\n",
    "gb_params = {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 4, 5]}\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(), gb_params, cv=5)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "best_gb = gb_grid.best_estimator_\n",
    "gb_pred_train = best_gb.predict(X_train)\n",
    "gb_pred_test = best_gb.predict(X_test)\n",
    "gb_train_mse = mean_squared_error(y_train, gb_pred_train)\n",
    "gb_test_mse = mean_squared_error(y_test, gb_pred_test)\n",
    "gb_train_r2 = r2_score(y_train, gb_pred_train)\n",
    "gb_test_r2 = r2_score(y_test, gb_pred_test)\n",
    "gb_feature_importance = rf_model.feature_importances_\n",
    "results_df = results_df.append({'Model': 'Gradient Boosting Regressor', 'Train MSE': gb_train_mse, 'Test MSE': gb_test_mse, 'Train R2': gb_train_r2, 'Test R2': gb_test_r2}, ignore_index=True)\n",
    "\n",
    "# Decision Tree Regressor with Hyperparameter Tuning\n",
    "dt_params = {'max_depth': [None, 10, 20]}\n",
    "dt_grid = GridSearchCV(DecisionTreeRegressor(), dt_params, cv=5)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "best_dt = dt_grid.best_estimator_\n",
    "dt_pred_train = best_dt.predict(X_train)\n",
    "dt_pred_test = best_dt.predict(X_test)\n",
    "dt_train_mse = mean_squared_error(y_train, dt_pred_train)\n",
    "dt_test_mse = mean_squared_error(y_test, dt_pred_test)\n",
    "dt_train_r2 = r2_score(y_train, dt_pred_train)\n",
    "dt_test_r2 = r2_score(y_test, dt_pred_test)\n",
    "results_df = results_df.append({'Model': 'Decision Tree Regressor', 'Train MSE': dt_train_mse, 'Test MSE': dt_test_mse, 'Train R2': dt_train_r2, 'Test R2': dt_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Bagging Regressor with Hyperparameter Tuning\n",
    "bag_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "bag_grid = GridSearchCV(BaggingRegressor(random_state=42), bag_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "bag_grid.fit(X_train, y_train)\n",
    "bag_best = bag_grid.best_estimator_\n",
    "\n",
    "# Using the best estimator from GridSearch to make predictions\n",
    "bag_pred_train = bag_best.predict(X_train)\n",
    "bag_pred_test = bag_best.predict(X_test)\n",
    "bag_train_mse = mean_squared_error(y_train, bag_pred_train)\n",
    "bag_test_mse = mean_squared_error(y_test, bag_pred_test)\n",
    "bag_train_r2 = r2_score(y_train, bag_pred_train)\n",
    "bag_test_r2 = r2_score(y_test, bag_pred_test)\n",
    "results_df = results_df.append({'Model': 'Bagging Regressor', 'Train MSE': bag_train_mse, 'Test MSE': bag_test_mse, 'Train R2': bag_train_r2, 'Test R2': bag_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# AdaBoost Regressor with Hyperparameter Tuning\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "ada_grid = GridSearchCV(AdaBoostRegressor(random_state=42), ada_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred_train = ada_model.predict(X_train)\n",
    "ada_pred_test = ada_model.predict(X_test)\n",
    "ada_train_mse = mean_squared_error(y_train, ada_pred_train)\n",
    "ada_test_mse = mean_squared_error(y_test, ada_pred_test)\n",
    "ada_train_r2 = r2_score(y_train, ada_pred_train)\n",
    "ada_test_r2 = r2_score(y_test, ada_pred_test)\n",
    "results_df = results_df.append({'Model': 'AdaBoost Regressor', 'Train MSE': ada_train_mse, 'Test MSE': ada_test_mse, 'Train R2': ada_train_r2, 'Test R2': ada_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n",
    "# Print the results DataFrame in tabulated form\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('23 TUN results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7da37-ffc1-440d-85f3-435d72d59058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank22_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank23_df2)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun', 'Target_Flowrate', 'Target_Phase_duration'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "# Initialize k-fold cross-validator\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Define the models to be evaluated\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "    SVR(),\n",
    "    MLPRegressor(),\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    BaggingRegressor(n_estimators=100, random_state=42),\n",
    "    AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "]\n",
    "\n",
    "# Iterate through each model and perform k-fold cross-validation\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    train_mse_list = []\n",
    "    test_mse_list = []\n",
    "    train_r2_list = []\n",
    "    test_r2_list = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        train_mse_list.append(train_mse)\n",
    "        test_mse_list.append(test_mse)\n",
    "        train_r2_list.append(train_r2)\n",
    "        test_r2_list.append(test_r2)\n",
    "    \n",
    "    mean_train_mse = sum(train_mse_list) / num_folds\n",
    "    mean_test_mse = sum(test_mse_list) / num_folds\n",
    "    mean_train_r2 = sum(train_r2_list) / num_folds\n",
    "    mean_test_r2 = sum(test_r2_list) / num_folds\n",
    "    \n",
    "    results_df = results_df.append({'Model': model_name, 'Train MSE': mean_train_mse, 'Test MSE': mean_test_mse,\n",
    "                                    'Train R2': mean_train_r2, 'Test R2': mean_test_r2}, ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('kfold_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02eba1-3624-4806-8cf9-e3009672a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank22_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank22_df2)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# List of regression models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "]\n",
    "\n",
    "# Apply PCR to each model\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    num_components = 5  # You can choose the number of principal components\n",
    "    pca = PCA(n_components=num_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Train the model with principal components\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    pred_train = model.predict(X_train_pca)\n",
    "    pred_test = model.predict(X_test_pca)\n",
    "    train_mse = mean_squared_error(y_train, pred_train)\n",
    "    test_mse = mean_squared_error(y_test, pred_test)\n",
    "    train_r2 = r2_score(y_train, pred_train)\n",
    "    test_r2 = r2_score(y_test, pred_test)\n",
    "\n",
    "    # Store results in the DataFrame\n",
    "    results_df = results_df.append({'Model': model_name, 'Train MSE': train_mse,\n",
    "                                    'Test MSE': test_mse, 'Train R2': train_r2, 'Test R2': test_r2},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('pcr_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb631528-79af-40fa-bdb7-de5e8bd9c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank22_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank22_df2)\n",
    "\n",
    "#Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN'\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)  # You can choose the number of neighbors\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred_train = knn_model.predict(X_train_scaled)\n",
    "knn_pred_test = knn_model.predict(X_test_scaled)\n",
    "knn_train_mse = mean_squared_error(y_train, knn_pred_train)\n",
    "knn_test_mse = mean_squared_error(y_test, knn_pred_test)\n",
    "knn_train_r2 = r2_score(y_train, knn_pred_train)\n",
    "knn_test_r2 = r2_score(y_test, knn_pred_test)\n",
    "results_df = results_df.append({'Model': 'K-Nearest Neighbors', 'Train MSE': knn_train_mse,\n",
    "                                'Test MSE': knn_test_mse, 'Train R2': knn_train_r2, 'Test R2': knn_test_r2},\n",
    "                               ignore_index=True)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVR(kernel='rbf')  # You can choose the kernel and tune other hyperparameters\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred_train = svm_model.predict(X_train_scaled)\n",
    "svm_pred_test = svm_model.predict(X_test_scaled)\n",
    "svm_train_mse = mean_squared_error(y_train, svm_pred_train)\n",
    "svm_test_mse = mean_squared_error(y_test, svm_pred_test)\n",
    "svm_train_r2 = r2_score(y_train, svm_pred_train)\n",
    "svm_test_r2 = r2_score(y_test, svm_pred_test)\n",
    "results_df = results_df.append({'Model': 'Support Vector Machine', 'Train MSE': svm_train_mse,\n",
    "                                'Test MSE': svm_test_mse, 'Train R2': svm_train_r2, 'Test R2': svm_test_r2},\n",
    "                               ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('knn_svm_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d3822-1a6e-4989-a27f-6b6ca68f8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank22_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank23_df2)\n",
    "\n",
    "# Define features and target\n",
    "#X = df.drop(['Phase_overrun', 'Target_Flowrate', 'Target_Phase_duration'], axis=1)\n",
    "#y = df['Phase_overrun']\n",
    "\n",
    "X = df.drop(['Phase_start_delay'], axis=1)\n",
    "y = df['Phase_start_delay']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "def build_simple_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the simple neural network\n",
    "simple_nn = build_simple_nn()\n",
    "simple_nn.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "pred_train_simple_nn = simple_nn.predict(X_train_scaled)\n",
    "pred_test_simple_nn = simple_nn.predict(X_test_scaled)\n",
    "train_mse_simple_nn = mean_squared_error(y_train, pred_train_simple_nn)\n",
    "test_mse_simple_nn = mean_squared_error(y_test, pred_test_simple_nn)\n",
    "train_r2_simple_nn = r2_score(y_train, pred_train_simple_nn)\n",
    "test_r2_simple_nn = r2_score(y_test, pred_test_simple_nn)\n",
    "results_df = results_df.append({'Model': 'Simple Neural Network', 'Train MSE': train_mse_simple_nn,\n",
    "                                'Test MSE': test_mse_simple_nn, 'Train R2': train_r2_simple_nn, 'Test R2': test_r2_simple_nn},\n",
    "                               ignore_index=True)\n",
    "\n",
    "# Define a Convolutional Neural Network (CNN)\n",
    "def build_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Reshape data for CNN (add an extra dimension for channels)\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Define a Convolutional Neural Network (CNN)\n",
    "def build_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the CNN\n",
    "cnn = build_cnn()\n",
    "cnn.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "pred_train_cnn = cnn.predict(X_train_reshaped)\n",
    "pred_test_cnn = cnn.predict(X_test_reshaped)\n",
    "train_mse_cnn = mean_squared_error(y_train, pred_train_cnn)\n",
    "test_mse_cnn = mean_squared_error(y_test, pred_test_cnn)\n",
    "train_r2_cnn = r2_score(y_train, pred_train_cnn)\n",
    "test_r2_cnn = r2_score(y_test, pred_test_cnn)\n",
    "results_df = results_df.append({'Model': 'Convolutional Neural Network', 'Train MSE': train_mse_cnn,\n",
    "                                'Test MSE': test_mse_cnn, 'Train R2': train_r2_cnn, 'Test R2': test_r2_cnn},\n",
    "                               ignore_index=True)\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('neural_network_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185e970-9435-4acf-93c3-3fdef2f3bafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804175d8-16a8-4280-b8c3-cc03aff1fb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
