{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c5d7f-5894-4e30-b871-8473ce3015bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agitation on Production tanks - 25mt10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351e04f5-c156-43f5-b04f-7aa88e1e6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Supress Warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "#The last line of code helps in suppressing the unnecessary warnings.\n",
    "from scipy import stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac75ad5-8489-43d2-a1c5-a7912cb763ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection:\n",
    "# Using the Specify Absolute Path: If the file is located in a different directory, you can specify the absolute path to the file when reading it using pd.read_csv():\n",
    "import pandas as pd\n",
    "file_path = r'C:\\Users\\User\\Desktop\\Thesis 2023\\Capstone---CCT\\Python Working Notebooks\\ProductionDataupdated1.csv'\n",
    "ProductionTank = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a42ef6-82d3-4d16-a479-1f451969c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProductionTank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf73d0c-7499-465d-8d22-5701d012c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductionTank.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d952e0-5808-4a32-bbe3-848e6474712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.DataFrame(ProductionTank) \n",
    "# Count the unique materialsProductionTank produced by each tank\n",
    "Batch_counts =ProductionTank.groupby('Tank_1')['BATCHID'].nunique().reset_index()\n",
    "#print(Batch_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89739e93-ebd2-4c5c-b275-d8375e01b6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAHwCAYAAAD9+W2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAojElEQVR4nO3de7RlVXkn7N9rcSlUFJWCIAUWKvqJXAosRUNjE7UFlYhix6gJitKiUT4vMfYwDo1KWtvR7SVtzKdCYwPG+w1J1PYKGjUiFHJHBbWUQgRE5SJioJjfH3udclOpos7hnF2nzqznGWOPs9ac6/LuRZ2xz48519rVWgsAAAD9udt8FwAAAMBkCHwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPYDNSVWdW1X+Z7zrWp6pOrqr/tgnP92dV9cUJHfsvqurqqrqpqu43iXPMVlUdXVXfWAjnr6o3VtU/boKaNtvfD4DNlcAHsIlV1aqq+u0QNq4egtQ957uu+VRVy6qqVdVWU22ttQ+21p44gXNtneQdSZ7YWrtna+26OTjm+H/Tqde7Z1/ttM8/ft7b16nlzzZVHXPB7wfA3BL4AObHH7fW7pnkgCQrkrxunuuZqKpaNN81jNk5yeIkF890xxrZ0GfnHw8Bcup13KyqnIHx8yb56Tq1fHBT1TGHtqjfD4BJEvgA5lFr7cokn0+y91jzA6rqm1V1Y1V9sap2nOqoqo9X1c+r6vqq+npVPXys78lVdcmw35VV9VdjfYdX1XlV9euq+lZV7bux2qpq/6o6dzjeRzMKSVN9/2663zBC9+Bh+eSqek9Vfa6qfpPkj6rqKVX13aq6oaquqKo3ju3+9eHnr4eRncese46q+sOqOnt472dX1R+O9Z1ZVX+7oes2tt1Dknx/7Fxfneax31xV30xyc5IHbuzarXPOB1XVV6vquqr6RVV9sKp2GOvfrao+VVXXDtu8e53931ZVv6qqH1fVk2Z47kdV1b8O/92vqqp3V9U2Y/2tql5cVZcN2/xDVdUGjvU/q+obVXXvDZxucVV9dLj+51bVfsN+r66qT65zrHdV1f/aWP2b8+8HwEIh8AHMo6raLcmTk3x3rPk5SZ6fZKck2yT5q7G+zyfZc+g7N8n46M1JSV7UWts+oz+Qp8LM/knen+RFSe6X5H1JTq+qbe+krm2SnJbkA0num+TjSZ4xw7f3nCRvTrJ9km8k+U2S5ybZIclTkvxFVT1t2Paxw88dhlGpf12nnvsm+WySdw3v4R1JPlt3vP/uzq5bkqS19oMkUyFgh9ba46Z57KOSHDu8l5/M6CokleS/J7l/kocl2S3JG4f3tSjJPw/HXJZk1yQfGdv3wIwC6o5J/keSkzYUyDZgTZJXDvs/Jsnjk7xknW0OT/LIJPsmeWaSQ+9QfNXdqurEof+JrbXrN3CuIzL6d3LfJB9KclqNps/+Y5LDpkJujabtPivJqRsrfnP9/QBYSAQ+gPlxWlX9OqMg9LUkbxnr+z+ttR+01n6b5GNJlk91tNbe31q7sbX2u4xCw35jIy63Jtmrqu7VWvtVa+3cof3YJO9rrZ3VWlvTWjslye+SPPpO6nt0kq2T/F1r7dbW2ieSnD3D9/iZ1to3W2u3t9Zuaa2d2Vq7cFi/IMmHk/zHaR7rKUkua619oLV2W2vtw0m+l+SPx7bZ4HWbg2Of3Fq7eOi/dQPHOW0YIZp6vTBJWmuXt9a+1Fr7XWvt2owC5dT7flRGQfDVrbXfDNdpfOT0J621E1tra5KckmSXjKakTktrbWVr7dtD3asyCjPrXvO3ttZ+3Vr7aZIzcsfrtnVG/53um9E0y5vv5HQrW2ufGK7POzIaEX50a+2qjEZw/2TY7rAkv2itrbyTY23uvx8AC4bABzA/ntZa26G19oDW2kuGP16n/Hxs+eYk90xGo0FV9daq+mFV3ZBk1bDN1JS2Z2Q0GvKTqvpaVT1maH9AkleNh5GMRpnufyf13T/Jla21NtY205GtK8ZXqurAqjpjmLp4fZIXj9W+Mfdfz/l/ktGI2JT1Xrc5OvYV2bip/6ZTrxOTpKp2rqqPDNMIb8hoxGvqfe+WUai7bQPHXPuexsLWtB9gUlUPqap/HqY53pBRcFr3mt/ZdXtwRiN3b2qt/dtGTrf2GrXWbk+yOr//N3ZKkj8flv88o5HjO7O5/34ALBgCH8DC8ZyM/vh+QpJ7ZzQFMBlNGUxr7ezW2hEZTWc7LaPRj2T0h/ib1wkjdx9GsjbkqiS7rjN9cPex5d8kufvUSlX9wXqO0dZZ/1CS05Ps1lq7d5L3TtW+nm3X9bOM/jAft3uSKzey33RM59gbq+/OvGXYf5/W2r0yCjxT7/uKJLvX2NNJ59h7Mhqt3HM492vHzj0dl2Y0ffLzVfXQjWy729RCjR5sszSja5uM/j3uW1V7ZzSFdBIPktmUvx8AC4bAB7BwbJ/RVLPrMgpba6e5VdU2NfreunsPU+puSHL70H1ikhcPI2xVVfeo0QNUtr+Tc/1rktuSvKyqtq6qIzOafjjl/CQPr6rlVbU4wz1p06j/l621W6rqURn9gT7l2qHeDT0Q5XNJHlJVz6mqrarqT5PsldH9b7M1yWMno/d9U5Lrq2rXJK8e6/tORuH6rcN/l8VVddAcnXfq3Dckuamq/p8kfzHTAwzB57VJvlxVD7qTTR9RVUcO4fUVGf1b/fZwjFuSfCKj0P+dYfroXNuUvx8AC4bAB7BwnJrRVMMrk1yS4Y/pMUclWTVMZ3txkj9LktbaOUlemOTdSX6V5PIkR9/ZiYbpe0cO2/0yyZ8m+dRY/w+SHJ/ky0kuy+heq415SZLjq+rGJH+T34+wTE1XfHOSbw7T6u5w/1QbfVfe4UleldEf9P81yeGttV9M47x3ag6P/U91x+/D+/TQ/qaMvl7g+oweDjN+HddkdK/ggzP6OoXVGV3rufJXGQXrGzMKNh+9KwcZ7ms7PslXq2rZBjb7TEa1/yqjf4tHrnO/4ylJ9snGp3PeVZvs9wNgIak73p4BADD3qmr3jKaX/kFr7Yb5rgdgS2GEDwCYqOGevr9M8hFhD2DTmtRN4gBs5oYRl0s20L3XhO6zYgtTVfdIcnVG0y0Pm+dyALY4pnQCAAB0ypROAACATgl8AAAAnVrw9/DtuOOObdmyZfNdBgAAwLxYuXLlL1prS9bXt+AD37Jly3LOOefMdxkAAADzoqp+sqE+UzoBAAA6JfABAAB0SuADAADo1IK/h299br311qxevTq33HLLfJfShcWLF2fp0qXZeuut57sUAABgBroMfKtXr87222+fZcuWparmu5wFrbWW6667LqtXr84ee+wx3+UAAAAz0OWUzltuuSX3u9/9hL05UFW53/3uZ7QUAAAWoC4DXxJhbw65lgAAsDB1G/gAAAC2dFtM4Fu0aFGWL1+evffeO3/yJ3+Sm2++OatWrcree+89r3UdffTR+cQnPjEnx1q1alU+9KEPrV0/55xz8rKXvWxOjg0AACw8W0zg22677XLeeefloosuyjbbbJP3vve9813SXXLbbbdtsG/dwLdixYq8613v2hRlAQAAm6EtJvCNO/jgg3P55ZcnSdasWZMXvvCFefjDH54nPvGJ+e1vf5skOfHEE/PIRz4y++23X57xjGfk5ptvTpJ8/OMfz95775399tsvj33sY9ce49WvfnUe+chHZt9998373ve+DZ67tZbjjjsuD33oQ/OEJzwh11xzzdq+ZcuW5Re/+EWS0ejcIYcckiR54xvfmKOOOioHHXRQjjrqqKxatSoHH3xwDjjggBxwwAH51re+lSR5zWtek3/5l3/J8uXL8853vjNnnnlmDj/88CTJL3/5yzztaU/Lvvvum0c/+tG54IIL1h77BS94QQ455JA88IEPFBABAKAjW1zgu+222/L5z38+++yzT5Lksssuy0tf+tJcfPHF2WGHHfLJT34ySXLkkUfm7LPPzvnnn5+HPexhOemkk5Ikxx9/fL7whS/k/PPPz+mnn54kOemkk3Lve987Z599ds4+++yceOKJ+fGPf7ze83/605/O97///VxyySU59dRT14a1jbnkkkvy5S9/OR/+8Iez00475Utf+lLOPffcfPSjH107bfOtb31rDj744Jx33nl55StfeYf93/CGN2T//ffPBRdckLe85S157nOfu7bve9/7Xr7whS/kO9/5Tt70pjfl1ltvncEVBQAANlddfg/f+vz2t7/N8uXLk4xG+I455pj87Gc/yx577LG2/RGPeERWrVqVJLnooovyute9Lr/+9a9z00035dBDD02SHHTQQTn66KPzzGc+M0ceeWSS5Itf/GIuuOCCtffiXX/99bnsssvW+711X//61/PsZz87ixYtyv3vf/887nGPm1b9T33qU7PddtslGX2x/HHHHZfzzjsvixYtyg9+8ION7v+Nb3xjbZh93OMel+uuuy433HBDkuQpT3lKtt1222y77bbZaaedcvXVV2fp0qXTqgsAANh8bTGBb+oevnVtu+22a5cXLVq0dkrn0UcfndNOOy377bdfTj755Jx55plJkve+970566yz8tnPfjaPeMQjsnLlyrTW8vd///drQ+FdtdVWW+X2229Pkn/3vXf3uMc91i6/853vzM4775zzzz8/t99+exYvXjyr8657De7sPkEAAGDh2OKmdE7XjTfemF122SW33nprPvjBD65t/+EPf5gDDzwwxx9/fJYsWZIrrrgihx56aN7znvesnQr5gx/8IL/5zW/We9zHPvax+ehHP5o1a9bkqquuyhlnnLG2b9myZVm5cmWSrB2NW5/rr78+u+yyS+52t7vlAx/4QNasWZMk2X777XPjjTeud5+DDz547fs488wzs+OOO+Ze97rXDK4IAACw0GwxI3wz9bd/+7c58MADs2TJkhx44IFrg9SrX/3qXHbZZWmt5fGPf3z222+/7Lvvvlm1alUOOOCAtNayZMmSnHbaaes97tOf/vR89atfzV577ZXdd989j3nMY9b2veENb8gxxxyT17/+9Wsf2LI+L3nJS/KMZzwjp556ag477LC1o3/77rtvFi1alP322y9HH3109t9//7X7TD2cZd99983d7373nHLKKbO/SAAAwGatWmvzXcOsrFixop1zzjl3aLv00kvzsIc9bJ4q6pNrCgAAm6eqWtlaW7G+PlM6AQAAOmVK54RceOGFOeqoo+7Qtu222+ass86ap4oAAIAtjcA3Ifvss896nwoKAPPhp8fvM98lACw4u//NhfNdwqyZ0gkAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACd6jbwLfQvlN+cuJYAALAwdRn4Fi9enOuuu05QmQOttVx33XVZvHjxfJcCAADMUJffw7d06dKsXr0611577XyX0oXFixdn6dKl810GAAAwQ10Gvq233jp77LHHfJcBAAAwr7qc0gkAAIDABwAA0C2BDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADo1EQDX1XtVlVnVNUlVXVxVb18aH9jVV1ZVecNryeP7fPXVXV5VX2/qg6dZH0AAAA922rCx78tyataa+dW1fZJVlbVl4a+d7bW3ja+cVXtleRZSR6e5P5JvlxVD2mtrZlwnQAAAN2Z6Ahfa+2q1tq5w/KNSS5Nsuud7HJEko+01n7XWvtxksuTPGqSNQIAAPRqk93DV1XLkuyf5Kyh6biquqCq3l9V9xnadk1yxdhuq7OegFhVx1bVOVV1zrXXXjvJsgEAABasTRL4quqeST6Z5BWttRuSvCfJg5IsT3JVkrfP5HittRNaaytaayuWLFky1+UCAAB0YeKBr6q2zijsfbC19qkkaa1d3Vpb01q7PcmJ+f20zSuT7Da2+9KhDQAAgBma9FM6K8lJSS5trb1jrH2Xsc2enuSiYfn0JM+qqm2rao8keyb5ziRrBAAA6NWkn9J5UJKjklxYVecNba9N8uyqWp6kJVmV5EVJ0lq7uKo+luSSjJ7w+VJP6AQAALhrJhr4WmvfSFLr6frcnezz5iRvnlhRAAAAW4hN9pROAAAANi2BDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE5tNd8FbCke8epT57sEgAVn5f987nyXAAALmhE+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANCpiQa+qtqtqs6oqkuq6uKqevnQft+q+lJVXTb8vM/QXlX1rqq6vKouqKoDJlkfAABAzyY9wndbkle11vZK8ugkL62qvZK8JslXWmt7JvnKsJ4kT0qy5/A6Nsl7JlwfAABAtyYa+FprV7XWzh2Wb0xyaZJdkxyR5JRhs1OSPG1YPiLJqW3k20l2qKpdJlkjAABArzbZPXxVtSzJ/knOSrJza+2qoevnSXYelndNcsXYbquHNgAAAGZokwS+qrpnkk8meUVr7YbxvtZaS9JmeLxjq+qcqjrn2muvncNKAQAA+jHxwFdVW2cU9j7YWvvU0Hz11FTN4ec1Q/uVSXYb233p0HYHrbUTWmsrWmsrlixZMrniAQAAFrBJP6WzkpyU5NLW2jvGuk5P8rxh+XlJPjPW/tzhaZ2PTnL92NRPAAAAZmCrCR//oCRHJbmwqs4b2l6b5K1JPlZVxyT5SZJnDn2fS/LkJJcnuTnJ8ydcHwAAQLcmGvhaa99IUhvofvx6tm9JXjrJmgAAALYUm+wpnQAAAGxaAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANCprWaycVUtSrLz+H6ttZ/OdVEAAADM3rQDX1X9v0nekOTqJLcPzS3JvhOoCwAAgFmayQjfy5M8tLV23aSKAQAAYO7M5B6+K5JcP6lCAAAAmFszGeH7UZIzq+qzSX431dhae8ecVwUAAMCszSTw/XR4bTO8AAAA2IxNO/C11t6UJFV1z2H9pkkVBQAAwOxN+x6+qtq7qr6b5OIkF1fVyqp6+ORKAwAAYDZm8tCWE5L8ZWvtAa21ByR5VZITJ1MWAAAAszWTwHeP1toZUyuttTOT3GPOKwIAAGBOzOgpnVX1+iQfGNb/PKMndwIAALAZmskI3wuSLEnyqeG1ZGgDAABgMzSTp3T+KsnLJlgLAAAAc2ijga+q/q619oqq+qckbd3+1tpTJ1IZAAAAszKdEb6pe/beNslCAAAAmFsbDXyttZXD4vLW2v8a76uqlyf52iQKAwAAYHZm8tCW562n7eg5qgMAAIA5Np17+J6d5DlJ9qiq08e6tk/yy0kVBgAAwOxM5x6+byW5KsmOSd4+1n5jkgsmURQAAACzN517+H6S5CdJHjP5cgAAAJgr076Hr6oeXVVnV9VNVfVvVbWmqm7YyD7vr6prquqisbY3VtWVVXXe8HryWN9fV9XlVfX9qjr0rr0lAAAAkpk9tOXdSZ6d5LIk2yX5L0n+YSP7nJzksPW0v7O1tnx4fS5JqmqvJM9K8vBhn/+vqhbNoD4AAADGzCTwpbV2eZJFrbU1rbX/k/WHufHtv57pP9jliCQfaa39rrX24ySXJ3nUTOoDAADg92YS+G6uqm2SnFdV/6OqXjnD/ccdV1UXDFM+7zO07ZrkirFtVg9tAAAA3AUzCWxHDdsfl+Q3SXZL8oy7cM73JHlQkuUZPf3z7Xe69XpU1bFVdU5VnXPttdfehRIAAAD6N63AN9xL95bW2i2ttRtaa29qrf3lMMVzRlprVw9TQm9PcmJ+P23zyoxC5JSlQ9v6jnFCa21Fa23FkiVLZloCAADAFmFaga+1tibJA4YpnbNSVbuMrT49ydQTPE9P8qyq2raq9kiyZ5LvzPZ8AAAAW6rpfPH6lB8l+WZVnZ7RlM4kSWvtHRvaoao+nOSQJDtW1eokb0hySFUtT9KSrEryouE4F1fVx5JckuS2JC8dgiYAAAB3wUwC3w+H192SbD+dHVprz15P80l3sv2bk7x5BjUBAACwAdMOfK21N02yEAAAAObWtANfVZ2R0TTMO2itPW5OKwIAAGBOzGRK51+NLS/O6CsZbpvbcgAAAJgrM5nSuXKdpm9WladoAgAAbKZmMqXzvmOrd0vyiCT3nvOKAAAAmBMzmdK5MqN7+CqjqZw/TnLMJIoCAABg9mYypXOPSRYCAADA3Npo4KuqI++sv7X2qbkrBwAAgLkynRG+Px5+7pTkD5N8dVj/oyTfSiLwAQAAbIY2Gvhaa89Pkqr6YpK9WmtXDeu7JDl5otUBAABwl91tBtvuNhX2Blcn2X2O6wEAAGCOzOQpnV+pqi8k+fCw/qdJvjz3JQEAADAXZvKUzuOq6ulJHjs0ndBa+/RkygIAAGC2ZjLClyHgrTfkVdW/ttYeMydVAQAAMGszuYdvYxbP4bEAAACYpbkMfG0OjwUAAMAszWXgAwAAYDMyl4Gv5vBYAAAAzNKMAl9VPaCqnjAsb1dV2491HzWnlQEAADAr0w58VfXCJJ9I8r6haWmS06b6W2sXzWllAAAAzMpMRvhemuSgJDckSWvtsiQ7TaIoAAAAZm8mge93rbV/m1qpqq3iyZwAAACbrZkEvq9V1WuTbFdV/ynJx5P802TKAgAAYLZmEvhek+TaJBcmeVGSzyV53SSKAgAAYPa2mu6GrbXbk5yY5MSqum+Spa01UzoBAAA2UzN5SueZVXWvIeytzCj4vXNypQEAADAbM5nSee/W2g1JjkxyamvtwCSPn0xZAAAAzNZMAt9WVbVLkmcm+ecJ1QMAAMAcmUngOz7JF5Jc3lo7u6oemOSyyZQFAADAbM3koS0fz+irGKbWf5TkGZMoCgAAgNmbduCrqsVJjkny8CSLp9pbay+YQF0AAADM0kymdH4gyR8kOTTJ15IsTXLjJIoCAABg9mYS+B7cWnt9kt+01k5J8pQkB06mLAAAAGZrJoHv1uHnr6tq7yT3TrLT3JcEAADAXJj2PXxJTqiq+yR5fZLTk9wzyd9MpCoAAABmbSZP6fzfw+LXkjxwMuUAAAAwV2bylM5tM/oahmXj+7XWjp/7sgAAAJitmUzp/EyS65OsTPK7yZQDAADAXJlJ4FvaWjtsYpUAAAAwp2bylM5vVdU+E6sEAACAObXREb6qujBJG7Z9flX9KKMpnZWktdb2nWyJAAAA3BXTmdJ5+MSrAAAAYM5NJ/BdneTFSR6c5MIkJ7XWbptoVQAAAMzadO7hOyXJiozC3pOSvH2iFQEAADAnpjPCt1drbZ8kqaqTknxnsiUBAAAwF6Yzwnfr1IKpnAAAAAvHdEb49quqG4blSrLdsD71lM57Taw6AAAA7rKNBr7W2qJNUQgAAABzayZfvA4AAMACIvABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANCpiQa+qnp/VV1TVReNtd23qr5UVZcNP+8ztFdVvauqLq+qC6rqgEnWBgAA0LtJj/CdnOSwddpek+QrrbU9k3xlWE+SJyXZc3gdm+Q9E64NAACgaxMNfK21ryf55TrNRyQ5ZVg+JcnTxtpPbSPfTrJDVe0yyfoAAAB6Nh/38O3cWrtqWP55kp2H5V2TXDG23eqh7d+pqmOr6pyqOufaa6+dXKUAAAAL2Lw+tKW11pK0u7DfCa21Fa21FUuWLJlAZQAAAAvffAS+q6emag4/rxnar0yy29h2S4c2AAAA7oL5CHynJ3nesPy8JJ8Za3/u8LTORye5fmzqJwAAADO01SQPXlUfTnJIkh2ranWSNyR5a5KPVdUxSX6S5JnD5p9L8uQklye5OcnzJ1kbAABA7yYa+Fprz95A1+PXs21L8tJJ1gMAALAlmdeHtgAAADA5Ah8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE5tNV8nrqpVSW5MsibJba21FVV13yQfTbIsyaokz2yt/Wq+agQAAFjI5nuE749aa8tbayuG9dck+Uprbc8kXxnWAQAAuAvmO/Ct64gkpwzLpyR52vyVAgAAsLDNZ+BrSb5YVSur6tihbefW2lXD8s+T7Dw/pQEAACx883YPX5L/0Fq7sqp2SvKlqvreeGdrrVVVW9+OQ0A8Nkl23333yVcKAACwAM3bCF9r7crh5zVJPp3kUUmurqpdkmT4ec0G9j2htbaitbZiyZIlm6pkAACABWVeAl9V3aOqtp9aTvLEJBclOT3J84bNnpfkM/NRHwAAQA/ma0rnzkk+XVVTNXyotfZ/q+rsJB+rqmOS/CTJM+epPgAAgAVvXgJfa+1HSfZbT/t1SR6/6SsCAADoz+b2tQwAAADMEYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBOCXwAAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPAACgUwIfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFMCHwAAQKcEPgAAgE4JfAAAAJ0S+AAAADol8AEAAHRK4AMAAOiUwAcAANApgQ8AAKBTAh8AAECnBD4AAIBObXaBr6oOq6rvV9XlVfWa+a4HAABgodqsAl9VLUryD0melGSvJM+uqr3mtyoAAICFabMKfEkeleTy1tqPWmv/luQjSY6Y55oAAAAWpM0t8O2a5Iqx9dVDGwAAADO01XwXcFdU1bFJjh1Wb6qq789nPbDA7ZjkF/NdBKxPve15810CsGn4LGLz9Iaa7wqm6wEb6tjcAt+VSXYbW186tN1Ba+2EJCdsqqKgZ1V1TmttxXzXAcCWy2cRTM7mNqXz7CR7VtUeVbVNkmclOX2eawIAAFiQNqsRvtbabVV1XJIvJFmU5P2ttYvnuSwAAIAFabMKfEnSWvtcks/Ndx2wBTE9GoD55rMIJqRaa/NdAwAAABOwud3DBwAAwBwR+KAzVbVbVZ1RVZdU1cVV9fKh/Y1VdWVVnTe8njy2z19X1eVV9f2qOnRoW1xV36mq84fjvGm+3hMAC8tcfRaN9S2qqu9W1T9v6vcCC50pndCZqtolyS6ttXOravskK5M8Lckzk9zUWnvbOtvvleTDSR6V5P5JvpzkIUluT3KP1tpNVbV1km8keXlr7dub7M0AsCDN1WdRa23N0P+XSVYkuVdr7fBN9kagA0b4oDOttataa+cOyzcmuTTJrneyyxFJPtJa+11r7cdJLk/yqDZy07DN1sPL/yECYKPm6rMoSapqaZKnJPnfk60a+iTwQceqalmS/ZOcNTQdV1UXVNX7q+o+Q9uuSa4Y22310DY1hea8JNck+VJr7awAwAzM9rMoyd8l+a8ZzTwBZkjgg05V1T2TfDLJK1prNyR5T5IHJVme5Kokb9/YMVpra1pry5MsTfKoqtp7YgUD0J3ZfhZV1eFJrmmtrZxwqdAtgQ86NNxz98kkH2ytfSpJWmtXDwHu9iQnZpgqk+TKJLuN7b50aFurtfbrJGckOWzCpQPQiTn6LDooyVOralWSjyR5XFX94yZ6C9AFgQ86U1WV5KQkl7bW3jHWvsvYZk9PctGwfHqSZ1XVtlW1R5I9k3ynqpZU1Q7Dvtsl+U9JvrcJ3gIAC9xcfRa11v66tba0tbYsybOSfLW19ueb5E1AJ7aa7wKAOXdQkqOSXDjcf5ckr03y7KpantGDV1YleVGStNYurqqPJbkkyW1JXtpaWzN8KJ9SVYsy+p9DH2uteRw2ANMxJ59Fm7hm6JKvZQAAAOiUKZ0AAACdEvgAAAA6JfABAAB0SuADAADolMAHAADQKYEPANZRVferqvOG18+r6sqx9W2meYxDqspXmQAwr3wPHwCso7V2XZLlSVJVb0xyU2vtbfNZEwDcFUb4AGAaquqFVXV2VZ1fVZ+sqrsP7SdX1buq6ltV9aOq+s/r2feRVfXdqnrQpq8cgC2ZwAcA0/Op1tojW2v7Jbk0yTFjfbsk+Q9JDk/y1vGdquoPk7w3yRGttR9uqmIBIDGlEwCma++q+m9JdkhyzyRfGOs7rbV2e5JLqmrnsfaHJTkhyRNbaz/bZJUCwMAIHwBMz8lJjmut7ZPkTUkWj/X9bmy5xpavSnJLkv0nXh0ArIfABwDTs32Sq6pq6yR/Ns19fp3kKUn+e1UdMpmyAGDDBD4AmJ7XJzkryTeTfG+6O7XWrs7o3r5/qKoDJ1QbAKxXtdbmuwYAAAAmwAgfAABApwQ+AACATgl8AAAAnRL4AAAAOiXwAQAAdErgAwAA6JTABwAA0CmBDwAAoFP/P9XRoIMG0VlrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tank_1    BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0    2503  107548288              60            0.0                  2   \n",
      "1    2503  107582580              91            1.0                  1   \n",
      "2    2503  107588998              61            1.0                  1   \n",
      "3    2503  107591594              91            1.0                  1   \n",
      "4    2503  107599585              62            2.0                490   \n",
      "..    ...        ...             ...            ...                ...   \n",
      "92   2504  108057688             135            0.0                276   \n",
      "93   2504  108067054              90            0.0                  3   \n",
      "94   2504  108083245              90            0.0                  1   \n",
      "95   2504  108083247              90            0.0                  0   \n",
      "96   2504  108091251              60            0.0                  3   \n",
      "\n",
      "    Quantity  Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0        0.0             0.0                  20.00              NaN  \n",
      "1        0.0             0.0                  30.00              NaN  \n",
      "2        0.0             0.0                  20.00              NaN  \n",
      "3        0.0             0.0                  30.00              NaN  \n",
      "4        0.0             0.0                  20.00              NaN  \n",
      "..       ...             ...                    ...              ...  \n",
      "92       0.0             0.0                  33.75              NaN  \n",
      "93       0.0             0.0                  30.00              NaN  \n",
      "94       0.0             0.0                  30.00              NaN  \n",
      "95       0.0             0.0                  30.00              NaN  \n",
      "96       0.0             0.0                  20.00              NaN  \n",
      "\n",
      "[97 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "data = pd.DataFrame(ProductionTank) \n",
    "\n",
    "\n",
    "specific_tanks = ['2503','2504']\n",
    "\n",
    "# Filter the dataframe for desired instruction steps\n",
    "desired_steps = ['STEP1_AGITATION', 'STEP2_AGITATION','STEP3_AGITATION']\n",
    "filtered_data = data[(data['Instruction_Step'].isin(desired_steps)) & (data['Tank_1'].isin(specific_tanks))]\n",
    "\n",
    "\n",
    "\n",
    "# Calculate total phase duration for each desired instruction step for each tank and material\n",
    "total_durations = filtered_data.groupby(['Tank_1', 'Material'])['Phase_duration'].sum().reset_index()\n",
    "\n",
    "# Present in table format\n",
    "#print(tabulate(total_durations, headers='keys', tablefmt='grid'))\n",
    "\n",
    "# Visualization using bar plots\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=total_durations, x='Tank_1', y='Phase_duration',ci=None)\n",
    "plt.title('Phase_duration for Each Tank by Phase')\n",
    "plt.ylabel('Phase_duration')\n",
    "plt.xlabel('Tank')\n",
    "plt.legend(title='Phase_duration')\n",
    "plt.show()\n",
    "\n",
    "#Aggregate data per tank\n",
    "aggregated_total_durations_df4 = filtered_data.groupby(['Tank_1','BATCHID']).agg({\n",
    "  #  'BATCHID': 'count',\n",
    "    # 'Material': 'count',\n",
    "    'Phase_duration': 'sum',\n",
    "    'Phase_overrun': 'sum',\n",
    "    'Phase_start_delay':'sum',\n",
    "    'Quantity':'sum',\n",
    "    'Flowrate_KGMIN':'sum',\n",
    "    'Target_Phase_duration':'mean',\n",
    "    'Target_Flowrate':'mean'\n",
    "}).reset_index()\n",
    "\n",
    " #Print the aggregated DataFrame\n",
    "print(aggregated_total_durations_df4)\n",
    "\n",
    "aggregated_total_durations_df4.to_csv('Agitation25MT10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a96ae54-4318-49bd-9508-ec5baf5f42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "#aggregated_total_durations_df4.dropna(inplace=True)  # Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a432170-b7e7-4193-ab58-f066eb7ca0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling du# Handling duplicates\n",
    "#aggregated_total_durations_df4.drop_duplicates(inplace=True)  # Remove duplicate rowsplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e85f8ce-adf8-4eaf-a000-2d08075992b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tank_1    BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0    2503  107548288              60            0.0                  2   \n",
      "1    2503  107582580              91            1.0                  1   \n",
      "2    2503  107588998              61            1.0                  1   \n",
      "3    2503  107591594              91            1.0                  1   \n",
      "5    2503  107610053              61            1.0                  2   \n",
      "..    ...        ...             ...            ...                ...   \n",
      "91   2504  108041496              60            0.0                  1   \n",
      "93   2504  108067054              90            0.0                  3   \n",
      "94   2504  108083245              90            0.0                  1   \n",
      "95   2504  108083247              90            0.0                  0   \n",
      "96   2504  108091251              60            0.0                  3   \n",
      "\n",
      "    Quantity  Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0        0.0             0.0                   20.0              NaN  \n",
      "1        0.0             0.0                   30.0              NaN  \n",
      "2        0.0             0.0                   20.0              NaN  \n",
      "3        0.0             0.0                   30.0              NaN  \n",
      "5        0.0             0.0                   20.0              NaN  \n",
      "..       ...             ...                    ...              ...  \n",
      "91       0.0             0.0                   20.0              NaN  \n",
      "93       0.0             0.0                   30.0              NaN  \n",
      "94       0.0             0.0                   30.0              NaN  \n",
      "95       0.0             0.0                   30.0              NaN  \n",
      "96       0.0             0.0                   20.0              NaN  \n",
      "\n",
      "[90 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define columns where you want to detect and remove outliers\n",
    "ProductionTank2510_df = pd.DataFrame(aggregated_total_durations_df4)\n",
    "#ProductionTank23_df\n",
    "columns_to_check = ['Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN', 'Target_Phase_duration']\n",
    "\n",
    "# Define a function to remove outliers using IQR\n",
    "def remove_outliers_iqr(data, column, iqr_multiplier=1.5):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers for each column\n",
    "for col in columns_to_check:\n",
    "   ProductionTank2510_df = remove_outliers_iqr(ProductionTank2510_df, col)\n",
    "# Display the cleaned DataFrame\n",
    "print(ProductionTank2510_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea6a804-d22e-41f5-9e07-92934bbc8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tank_1    BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "0    2503  107548288       -0.914296      -0.837404           0.981533   \n",
      "1    2503  107582580        0.485751       1.095066          -0.212223   \n",
      "2    2503  107588998       -0.869133       1.095066          -0.212223   \n",
      "3    2503  107591594        0.485751       1.095066          -0.212223   \n",
      "5    2503  107610053       -0.869133       1.095066           0.981533   \n",
      "..    ...        ...             ...            ...                ...   \n",
      "91   2504  108041496       -0.914296      -0.837404          -0.212223   \n",
      "93   2504  108067054        0.440588      -0.837404           2.175288   \n",
      "94   2504  108083245        0.440588      -0.837404          -0.212223   \n",
      "95   2504  108083247        0.440588      -0.837404          -1.405979   \n",
      "96   2504  108091251       -0.914296      -0.837404           2.175288   \n",
      "\n",
      "    Quantity  Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "0        0.0             0.0                   20.0              NaN  \n",
      "1        0.0             0.0                   30.0              NaN  \n",
      "2        0.0             0.0                   20.0              NaN  \n",
      "3        0.0             0.0                   30.0              NaN  \n",
      "5        0.0             0.0                   20.0              NaN  \n",
      "..       ...             ...                    ...              ...  \n",
      "91       0.0             0.0                   20.0              NaN  \n",
      "93       0.0             0.0                   30.0              NaN  \n",
      "94       0.0             0.0                   30.0              NaN  \n",
      "95       0.0             0.0                   30.0              NaN  \n",
      "96       0.0             0.0                   20.0              NaN  \n",
      "\n",
      "[90 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Scaling numerical variables (if needed)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['Phase_duration', 'Phase_overrun', 'Phase_start_delay', 'Flowrate_KGMIN']\n",
    "ProductionTank2510_df[numerical_cols] = scaler.fit_transform(ProductionTank2510_df[numerical_cols])\n",
    "print(ProductionTank2510_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58871e9d-b507-493d-8245-6443bda1dd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Summary Statistics:\n",
      "            BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "count  9.700000e+01       97.000000      97.000000          97.000000   \n",
      "mean   1.078264e+08       81.721649       0.453608          17.319588   \n",
      "std    1.635974e+05       23.271380       0.540459          71.455631   \n",
      "min    1.075483e+08       30.000000       0.000000           0.000000   \n",
      "25%    1.076604e+08       61.000000       0.000000           1.000000   \n",
      "50%    1.078292e+08       90.000000       0.000000           1.000000   \n",
      "75%    1.079634e+08       91.000000       1.000000           2.000000   \n",
      "max    1.080913e+08      146.000000       2.000000         490.000000   \n",
      "\n",
      "       Quantity  Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "count      97.0            97.0              97.000000              0.0  \n",
      "mean        0.0             0.0              26.589347              NaN  \n",
      "std         0.0             0.0               5.467950              NaN  \n",
      "min         0.0             0.0              15.000000              NaN  \n",
      "25%         0.0             0.0              20.000000              NaN  \n",
      "50%         0.0             0.0              30.000000              NaN  \n",
      "75%         0.0             0.0              30.000000              NaN  \n",
      "max         0.0             0.0              36.250000              NaN  \n",
      "\n",
      "Cleaned DataFrame Summary Statistics:\n",
      "            BATCHID  Phase_duration  Phase_overrun  Phase_start_delay  \\\n",
      "count  9.000000e+01    9.000000e+01   9.000000e+01       9.000000e+01   \n",
      "mean   1.078336e+08    1.912051e-16  -1.233581e-17      -1.973730e-17   \n",
      "std    1.621145e+05    1.005602e+00   1.005602e+00       1.005602e+00   \n",
      "min    1.075483e+08   -2.269179e+00  -8.374036e-01      -1.405979e+00   \n",
      "25%    1.076863e+08   -8.691328e-01  -8.374036e-01      -2.122232e-01   \n",
      "50%    1.078336e+08    4.405881e-01  -8.374036e-01      -2.122232e-01   \n",
      "75%    1.079754e+08    4.405881e-01   1.095066e+00       9.815325e-01   \n",
      "max    1.080913e+08    2.518076e+00   3.027536e+00       2.175288e+00   \n",
      "\n",
      "       Quantity  Flowrate_KGMIN  Target_Phase_duration  Target_Flowrate  \n",
      "count      90.0            90.0              90.000000              0.0  \n",
      "mean        0.0             0.0              26.324074              NaN  \n",
      "std         0.0             0.0               5.435389              NaN  \n",
      "min         0.0             0.0              15.000000              NaN  \n",
      "25%         0.0             0.0              20.000000              NaN  \n",
      "50%         0.0             0.0              30.000000              NaN  \n",
      "75%         0.0             0.0              30.000000              NaN  \n",
      "max         0.0             0.0              33.750000              NaN  \n"
     ]
    }
   ],
   "source": [
    "# For the original DataFrame\n",
    "print(\"Original DataFrame Summary Statistics:\")\n",
    "print(aggregated_total_durations_df4.describe())\n",
    "\n",
    "# After removing outliers\n",
    "print(\"\\nCleaned DataFrame Summary Statistics:\")\n",
    "print(ProductionTank2510_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32eccb79-e88b-4cd5-baa7-968f0f166525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|    | Model                       |   Train MSE |   Test MSE |   Train R2 |     Test R2 |\n",
      "+====+=============================+=============+============+============+=============+\n",
      "|  0 | Linear Regression           | 0.693827    |  0.907205  |   0.319334 |  0.0161356  |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  1 | Ridge Regression            | 0.694049    |  0.915751  |   0.319116 |  0.00686846 |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  2 | Lasso Regression            | 0.904533    |  1.09825   |   0.112624 | -0.191054   |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  3 | Random Forest Regressor     | 0.159922    |  0.151426  |   0.843112 |  0.835779   |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  4 | Gradient Boosting Regressor | 0.0106496   |  0.091642  |   0.989552 |  0.900614   |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  5 | Decision Tree Regressor     | 0.0760719   |  0.13278   |   0.925371 |  0.856      |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  6 | Bagging Regressor           | 0.0369035   |  0.137863  |   0.963796 |  0.850487   |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  7 | AdaBoost Regressor          | 0.0337291   |  0.0825311 |   0.966911 |  0.910495   |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n",
      "|  8 | Extra Trees Regressor       | 8.97398e-31 |  0.542075  |   1        |  0.41212    |\n",
      "+----+-----------------------------+-------------+------------+------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load your dataset (replace 'ProductionTank2203_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank2510_df)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun','Target_Flowrate'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "# Define features and target\n",
    "#X = df.drop(['Phase_overrun','Flowrate_KGMIN','Target_Phase_duration','Target_Flowrate','Phase_start_delay'], axis=1)\n",
    "#y = df['Phase_overrun']\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred_train = lr_model.predict(X_train)\n",
    "lr_pred_test = lr_model.predict(X_test)\n",
    "lr_train_mse = mean_squared_error(y_train, lr_pred_train)\n",
    "lr_test_mse = mean_squared_error(y_test, lr_pred_test)\n",
    "lr_train_r2 = r2_score(y_train, lr_pred_train)\n",
    "lr_test_r2 = r2_score(y_test, lr_pred_test)\n",
    "results_df = results_df.append({'Model': 'Linear Regression', 'Train MSE': lr_train_mse, 'Test MSE': lr_test_mse, 'Train R2': lr_train_r2, 'Test R2': lr_test_r2}, ignore_index=True)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ridge_pred_train = ridge_model.predict(X_train)\n",
    "ridge_pred_test = ridge_model.predict(X_test)\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_pred_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_pred_test)\n",
    "ridge_train_r2 = r2_score(y_train, ridge_pred_train)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_pred_test)\n",
    "results_df = results_df.append({'Model': 'Ridge Regression', 'Train MSE': ridge_train_mse, 'Test MSE': ridge_test_mse, 'Train R2': ridge_train_r2, 'Test R2': ridge_test_r2}, ignore_index=True)\n",
    "\n",
    "# Lasso Regression\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "lasso_pred_train = lasso_model.predict(X_train)\n",
    "lasso_pred_test = lasso_model.predict(X_test)\n",
    "lasso_train_mse = mean_squared_error(y_train, lasso_pred_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_pred_test)\n",
    "lasso_train_r2 = r2_score(y_train, lasso_pred_train)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_pred_test)\n",
    "results_df = results_df.append({'Model': 'Lasso Regression', 'Train MSE': lasso_train_mse, 'Test MSE': lasso_test_mse, 'Train R2': lasso_train_r2, 'Test R2': lasso_test_r2}, ignore_index=True)\n",
    "\n",
    "# RandomForest Regressor\n",
    "#rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=10, min_samples_split=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred_train = rf_model.predict(X_train)\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "rf_train_mse = mean_squared_error(y_train, rf_pred_train)\n",
    "rf_test_mse = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_train_r2 = r2_score(y_train, rf_pred_train)\n",
    "rf_test_r2 = r2_score(y_test, rf_pred_test)\n",
    "results_df = results_df.append({'Model': 'Random Forest Regressor', 'Train MSE': rf_train_mse, 'Test MSE': rf_test_mse, 'Train R2': rf_train_r2, 'Test R2': rf_test_r2}, ignore_index=True)\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "#gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.05, max_depth=5, subsample=0.8, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred_train = gb_model.predict(X_train)\n",
    "gb_pred_test = gb_model.predict(X_test)\n",
    "gb_train_mse = mean_squared_error(y_train, gb_pred_train)\n",
    "gb_test_mse = mean_squared_error(y_test, gb_pred_test)\n",
    "gb_train_r2 = r2_score(y_train, gb_pred_train)\n",
    "gb_test_r2 = r2_score(y_test, gb_pred_test)\n",
    "results_df = results_df.append({'Model': 'Gradient Boosting Regressor', 'Train MSE': gb_train_mse, 'Test MSE': gb_test_mse, 'Train R2': gb_train_r2, 'Test R2': gb_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Decision Tree Regressor\n",
    "#dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=10, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred_train = dt_model.predict(X_train)\n",
    "dt_pred_test = dt_model.predict(X_test)\n",
    "dt_train_mse = mean_squared_error(y_train, dt_pred_train)\n",
    "dt_test_mse = mean_squared_error(y_test, dt_pred_test)\n",
    "dt_train_r2 = r2_score(y_train, dt_pred_train)\n",
    "dt_test_r2 = r2_score(y_test, dt_pred_test)\n",
    "results_df = results_df.append({'Model': 'Decision Tree Regressor', 'Train MSE': dt_train_mse, 'Test MSE': dt_test_mse, 'Train R2': dt_train_r2, 'Test R2': dt_test_r2}, ignore_index=True)\n",
    "\n",
    "# Bagging Regressor (based on Decision Trees by default)\n",
    "bag_model = BaggingRegressor(n_estimators=100, random_state=42)\n",
    "bag_model.fit(X_train, y_train)\n",
    "bag_pred_train = bag_model.predict(X_train)\n",
    "bag_pred_test = bag_model.predict(X_test)\n",
    "bag_train_mse = mean_squared_error(y_train, bag_pred_train)\n",
    "bag_test_mse = mean_squared_error(y_test, bag_pred_test)\n",
    "bag_train_r2 = r2_score(y_train, bag_pred_train)\n",
    "bag_test_r2 = r2_score(y_test, bag_pred_test)\n",
    "results_df = results_df.append({'Model': 'Bagging Regressor', 'Train MSE': bag_train_mse, 'Test MSE': bag_test_mse, 'Train R2': bag_train_r2, 'Test R2': bag_test_r2}, ignore_index=True)\n",
    "\n",
    "# AdaBoost Regressor\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred_train = ada_model.predict(X_train)\n",
    "ada_pred_test = ada_model.predict(X_test)\n",
    "ada_train_mse = mean_squared_error(y_train, ada_pred_train)\n",
    "ada_test_mse = mean_squared_error(y_test, ada_pred_test)\n",
    "ada_train_r2 = r2_score(y_train, ada_pred_train)\n",
    "ada_test_r2 = r2_score(y_test, ada_pred_test)\n",
    "results_df = results_df.append({'Model': 'AdaBoost Regressor', 'Train MSE': ada_train_mse, 'Test MSE': ada_test_mse, 'Train R2': ada_train_r2, 'Test R2': ada_test_r2}, ignore_index=True)\n",
    "\n",
    "# Extra Trees Regressor\n",
    "et_model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "et_model.fit(X_train, y_train)\n",
    "et_pred_train = et_model.predict(X_train)\n",
    "et_pred_test = et_model.predict(X_test)\n",
    "et_train_mse = mean_squared_error(y_train, et_pred_train)\n",
    "et_test_mse = mean_squared_error(y_test, et_pred_test)\n",
    "et_train_r2 = r2_score(y_train, et_pred_train)\n",
    "et_test_r2 = r2_score(y_test, et_pred_test)\n",
    "results_df = results_df.append({'Model': 'Extra Trees Regressor', 'Train MSE': et_train_mse, 'Test MSE': et_test_mse, 'Train R2': et_train_r2, 'Test R2': et_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the results DataFrame\n",
    "#print(results_df)\n",
    "# Print the results DataFrame in tabulated form\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('2510AGresults.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f92c41cc-63b5-4881-9886-35d8d3f00469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression:\n",
      "  Mean MSE: 0.855624\n",
      "  Std MSE: 0.183167\n",
      "\n",
      "Ridge:\n",
      "  Mean MSE: 0.853372\n",
      "  Std MSE: 0.182794\n",
      "\n",
      "Lasso:\n",
      "  Mean MSE: 1.130102\n",
      "  Std MSE: 0.201885\n",
      "\n",
      "RandomForestRegressor:\n",
      "  Mean MSE: 0.247000\n",
      "  Std MSE: 0.117201\n",
      "\n",
      "GradientBoostingRegressor:\n",
      "  Mean MSE: 0.113123\n",
      "  Std MSE: 0.071372\n",
      "\n",
      "SVR:\n",
      "  Mean MSE: 1.543758\n",
      "  Std MSE: 0.558364\n",
      "\n",
      "MLPRegressor:\n",
      "  Mean MSE: 7624171409010.271484\n",
      "  Std MSE: 5462246928516.617188\n",
      "\n",
      "DecisionTreeRegressor:\n",
      "  Mean MSE: 0.373444\n",
      "  Std MSE: 0.462055\n",
      "\n",
      "AdaBoostRegressor:\n",
      "  Mean MSE: 0.224950\n",
      "  Std MSE: 0.143045\n",
      "\n",
      "BaggingRegressor:\n",
      "  Mean MSE: 0.207017\n",
      "  Std MSE: 0.074682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of models with their respective hyperparameters\n",
    "# Initialize models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    SVR(),\n",
    "    MLPRegressor(),\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    BaggingRegressor(n_estimators=100, random_state=42)\n",
    "]\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse_scores = -scores  # Convert negative MSE back to positive\n",
    "    mean_mse = mse_scores.mean()\n",
    "    std_mse = mse_scores.std()\n",
    "    print(f\"{model_name}:\\n  Mean MSE: {mean_mse:.6f}\\n  Std MSE: {std_mse:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "245d25b8-fb9d-46b2-8aef-b2c04bdf2390",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-06440ac220b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Linear Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mlr_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mlr_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mlr_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    685\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1072\u001b[0m         )\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1075\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 )\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Load your dataset (replace 'ProductionTank2202_df2' with your actual dataset)\n",
    "df = pd.DataFrame(ProductionTank2510_df)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Phase_overrun','Target_Flowrate'], axis=1)\n",
    "y = df['Phase_overrun']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Train MSE', 'Test MSE', 'Train R2', 'Test R2'])\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred_train = lr_model.predict(X_train)\n",
    "lr_pred_test = lr_model.predict(X_test)\n",
    "lr_train_mse = mean_squared_error(y_train, lr_pred_train)\n",
    "lr_test_mse = mean_squared_error(y_test, lr_pred_test)\n",
    "lr_train_r2 = r2_score(y_train, lr_pred_train)\n",
    "lr_test_r2 = r2_score(y_test, lr_pred_test)\n",
    "results_df = results_df.append({'Model': 'Linear Regression', 'Train MSE': lr_train_mse, 'Test MSE': lr_test_mse, 'Train R2': lr_train_r2, 'Test R2': lr_test_r2}, ignore_index=True)\n",
    "\n",
    "# Ridge Regression with Hyperparameter Tuning\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_\n",
    "ridge_pred_train = best_ridge.predict(X_train)\n",
    "ridge_pred_test = best_ridge.predict(X_test)\n",
    "ridge_train_mse = mean_squared_error(y_train, ridge_pred_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, ridge_pred_test)\n",
    "ridge_train_r2 = r2_score(y_train, ridge_pred_train)\n",
    "ridge_test_r2 = r2_score(y_test, ridge_pred_test)\n",
    "results_df = results_df.append({'Model': 'Ridge Regression', 'Train MSE': ridge_train_mse, 'Test MSE': ridge_test_mse, 'Train R2': ridge_train_r2, 'Test R2': ridge_test_r2}, ignore_index=True)\n",
    "\n",
    "# Lasso Regression with Hyperparameter Tuning\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0]}\n",
    "lasso_grid = GridSearchCV(Lasso(), lasso_params, cv=5)\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "lasso_pred_train = best_lasso.predict(X_train)\n",
    "lasso_pred_test = best_lasso.predict(X_test)\n",
    "lasso_train_mse = mean_squared_error(y_train, lasso_pred_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, lasso_pred_test)\n",
    "lasso_train_r2 = r2_score(y_train, lasso_pred_train)\n",
    "lasso_test_r2 = r2_score(y_test, lasso_pred_test)\n",
    "results_df = results_df.append({'Model': 'Lasso Regression', 'Train MSE': lasso_train_mse, 'Test MSE': lasso_test_mse, 'Train R2': lasso_train_r2, 'Test R2': lasso_test_r2}, ignore_index=True)\n",
    "\n",
    "# Random Forest Regressor with Hyperparameter Tuning\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20]}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(), rf_params, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "rf_pred_train = best_rf.predict(X_train)\n",
    "rf_pred_test = best_rf.predict(X_test)\n",
    "rf_train_mse = mean_squared_error(y_train, rf_pred_train)\n",
    "rf_test_mse = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_train_r2 = r2_score(y_train, rf_pred_train)\n",
    "rf_test_r2 = r2_score(y_test, rf_pred_test)\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "results_df = results_df.append({'Model': 'Random Forest Regressor', 'Train MSE': rf_train_mse, 'Test MSE': rf_test_mse, 'Train R2': rf_train_r2, 'Test R2': rf_test_r2}, ignore_index=True)\n",
    "\n",
    "# Gradient Boosting Regressor with Hyperparameter Tuning\n",
    "gb_params = {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 4, 5]}\n",
    "gb_grid = GridSearchCV(GradientBoostingRegressor(), gb_params, cv=5)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "best_gb = gb_grid.best_estimator_\n",
    "gb_pred_train = best_gb.predict(X_train)\n",
    "gb_pred_test = best_gb.predict(X_test)\n",
    "gb_train_mse = mean_squared_error(y_train, gb_pred_train)\n",
    "gb_test_mse = mean_squared_error(y_test, gb_pred_test)\n",
    "gb_train_r2 = r2_score(y_train, gb_pred_train)\n",
    "gb_test_r2 = r2_score(y_test, gb_pred_test)\n",
    "gb_feature_importance = rf_model.feature_importances_\n",
    "results_df = results_df.append({'Model': 'Gradient Boosting Regressor', 'Train MSE': gb_train_mse, 'Test MSE': gb_test_mse, 'Train R2': gb_train_r2, 'Test R2': gb_test_r2}, ignore_index=True)\n",
    "\n",
    "# Decision Tree Regressor with Hyperparameter Tuning\n",
    "dt_params = {'max_depth': [None, 10, 20]}\n",
    "dt_grid = GridSearchCV(DecisionTreeRegressor(), dt_params, cv=5)\n",
    "dt_grid.fit(X_train, y_train)\n",
    "best_dt = dt_grid.best_estimator_\n",
    "dt_pred_train = best_dt.predict(X_train)\n",
    "dt_pred_test = best_dt.predict(X_test)\n",
    "dt_train_mse = mean_squared_error(y_train, dt_pred_train)\n",
    "dt_test_mse = mean_squared_error(y_test, dt_pred_test)\n",
    "dt_train_r2 = r2_score(y_train, dt_pred_train)\n",
    "dt_test_r2 = r2_score(y_test, dt_pred_test)\n",
    "results_df = results_df.append({'Model': 'Decision Tree Regressor', 'Train MSE': dt_train_mse, 'Test MSE': dt_test_mse, 'Train R2': dt_train_r2, 'Test R2': dt_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# Bagging Regressor with Hyperparameter Tuning\n",
    "bag_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "bag_grid = GridSearchCV(BaggingRegressor(random_state=42), bag_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "bag_grid.fit(X_train, y_train)\n",
    "bag_best = bag_grid.best_estimator_\n",
    "\n",
    "# Using the best estimator from GridSearch to make predictions\n",
    "bag_pred_train = bag_best.predict(X_train)\n",
    "bag_pred_test = bag_best.predict(X_test)\n",
    "bag_train_mse = mean_squared_error(y_train, bag_pred_train)\n",
    "bag_test_mse = mean_squared_error(y_test, bag_pred_test)\n",
    "bag_train_r2 = r2_score(y_train, bag_pred_train)\n",
    "bag_test_r2 = r2_score(y_test, bag_pred_test)\n",
    "results_df = results_df.append({'Model': 'Bagging Regressor', 'Train MSE': bag_train_mse, 'Test MSE': bag_test_mse, 'Train R2': bag_train_r2, 'Test R2': bag_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "# AdaBoost Regressor with Hyperparameter Tuning\n",
    "ada_model = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "ada_grid = GridSearchCV(AdaBoostRegressor(random_state=42), ada_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred_train = ada_model.predict(X_train)\n",
    "ada_pred_test = ada_model.predict(X_test)\n",
    "ada_train_mse = mean_squared_error(y_train, ada_pred_train)\n",
    "ada_test_mse = mean_squared_error(y_test, ada_pred_test)\n",
    "ada_train_r2 = r2_score(y_train, ada_pred_train)\n",
    "ada_test_r2 = r2_score(y_test, ada_pred_test)\n",
    "results_df = results_df.append({'Model': 'AdaBoost Regressor', 'Train MSE': ada_train_mse, 'Test MSE': ada_test_mse, 'Train R2': ada_train_r2, 'Test R2': ada_test_r2}, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results_df)\n",
    "# Print the results DataFrame in tabulated form\n",
    "print(tabulate(results_df, headers='keys', tablefmt='grid'))\n",
    "# Save results DataFrame to an Excel file\n",
    "results_df.to_excel('2510AGresults.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f084a9-4d0e-4bde-a378-b69ef1f12cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
